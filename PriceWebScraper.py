# -*- coding: utf-8 -*-
"""The purpose of this module is solely to implement priceWebScraper"""

import time
import datetime
import re
from configparser import SafeConfigParser
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import pandas as pd
from support import *

class priceWebScraper():

    def scrapeLocationSwitch(website):

        """
        Implements a dictionary that contains the location of the
        price in the html structure of each website. Used in scrapePrices

        Args:
            website: Website name String (e.g. esprit.de)

        Returns: A string tuple containing location data for the scraper
        """
        switch = {
            'esprit.de': ('span', 'class', 'spv-price__selling'),
            'hm.com': ('span', 'class', 'price'),                                   #has severe bot protection
            'zalando.de': ('h4', 'class', 'h-text h-color-red title-3 h-p-top-m'),
            'cyberport.de': ('div', 'class','price orange'),
            'notebooksbilliger.de': ('text', 'class', 'nbb-svg-outline'),           #has bot protection
            'mediamarkt.de': ('div', 'class', 'price'),
            'tchibo.de': ('span', 'itemprop', 'price'),
            'conrad.de': ('div', 'itemprop', 'price'),                              #has bot protection
            'otto.de': ('span', 'id', 'normalPriceAmount')}
        return switch.get(website)

    def scrapePrices(driverArray):

        """
        Consists of three loops:
        -The outer loop will iterate through all specified sites
        -The middle loop will iterate through all given drivers from automisedProfileValidation
        -The inner loop will visit shoppin-urls as specified in the config, uses BeautifulSoup to scrape
        the sites context, filters for the price and uses Pandas to build a .csv containing a DataFrame
        of prices in relation to the url to analyse it later.

        Args:
            driverArray: Array of Selenium webdrivers generated by automisedProfileValidation

        Returns: saves dataframe.csv file for each specified website to the specified location of scraperResults
        """
        #load config
        _config = SafeConfigParser()
        _config.read('config.ini')
        _scrapeSitesArray = _config.get('pages', 'scrape').split()
        _profileDirectory = _config.get('directories', 'profileDirectory')
        _scrapeDirectory = _config.get('directories', 'scrapeDirectory')

        for _scrape in _scrapeSitesArray:
            #This datalist will contain datalists of the users scraped product prices
            _metaDatalist = []

            #for every user a webdriver will be started with the set Firefox profile and proxy from the config
            for _driver in driverArray:

                #this datalist will contain the scraped prices of one user for each product as an array.
                _datalist = []
                #each website URL will be called
                _url = "https://www." + _scrape
                _driver.get(_url)
                time.sleep(2)

                _products = _config.get('pages', _scrape).split()

                for _prod in _products:

                    if (_prod[0]=="/"):
                        _driver.get(_url + _prod)
                    else:
                        _driver.get(_prod)

                    time.sleep(2)
                    #BeautifulSoup will scrape the Html source page and filter for set words individually for each site
                    _soup=BeautifulSoup(_driver.page_source, "html.parser")
                    #get the location of the price within the html structure
                    _scrapeLocation = priceWebScraper.scrapeLocationSwitch(_scrape)
                    _location = _scrapeLocation[0]
                    _type = _scrapeLocation[1]
                    _name = _scrapeLocation[2]

                    _priceBox = _soup.find(_location, attrs={_type:_name})

                    #_priceBox = soup.find('span', attrs={'class':'price'}) #H&M, has severe bot protection
                    #_priceBox = soup.find('h4', attrs={'class':'h-text h-color-red title-3 h-p-top-m'}) #zalando
                    #_priceBox = soup.find('div', attrs={'class':'price orange'}) #cyberport
                    #_priceBox = soup.find('text', attrs={'class':'nbb-svg-outline'}) #notebooksbilliger, has bot protection
                    #_priceBox = soup.find('div', attrs={'itemprop':'price'}) #conrad, has bot protection
                    #_priceBox = soup.find('span', attrs={'itemprop':'price'}) #tchibo
                    #_priceBox = soup.find('div', attrs={'class':'price'}) #mediamarkt
                    #_priceBox = soup.find('span', attrs={'id':'normalPriceAmount'}) #otto
                    #Get only the price from the html line that contains it

                    _price = "00,00"
                    try:
                        _price = _priceBox.text
                        _middle = _price.find(",")
                        _loop = True
                        i = 1
                        while(_loop == True):
                            print(_price[_middle-i])
                            if (_price[_middle-i] in ['0','1','2','3','4','5','6','7','8','9']):
                                i = i + 1
                            else:
                                print("false")
                                _loop = False
                                
                        _price = _price[_middle-i:_middle+3]
                        print(_price)
                    except Exception:
                        print ("No price found")

                    _datalist.append(_price)

                _metaDatalist.append(_datalist)

            _datetime = str(datetime.datetime.now())
            _datetime = _datetime[:19]
            _datetime = _datetime.replace(":", ".")
            #Build a DataFrame, transpose it and save it using Pandas
            _df = pd.DataFrame(_metaDatalist, columns=_products).T
            _df.to_csv((_scrapeDirectory + _datetime + " " +_scrape + ' .csv'), sep='\t', encoding='utf-8')
            print("Dataframe for " + _scrape + " generated")
