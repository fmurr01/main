# -*- coding: utf-8 -*-
"""The purpose of this module is solely to implement priceWebScraper"""

import os
import time
import datetime
import re
from configparser import SafeConfigParser
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
from bs4 import BeautifulSoup
import pandas as pd
from support import *

class priceWebScraper():

    def scrapeLocationSwitch(website):

        """
        Implements a dictionary that contains the location of the
        price in the html structure of each website. Used in scrapePrices

        Args:
            website: Website name String (e.g. esprit.de)

        Returns: A string tuple containing location data for the scraper
        """
        switch = {
            'esprit.de': ('span', 'class', 'spv-price__selling'),
            'hm.com': ('span', 'class', 'price'),                                   #has severe bot protection
            'zalando.de': ('h4', 'class', 'h-text h-color-red title-3 h-p-top-m', "h-text h-color-black title-3 h-p-top-m"),
            'cyberport.de': ('div', 'class','price orange', 'price '),
            'notebooksbilliger.de': ('text', 'class', 'nbb-svg-base'),              #has bot protection
            'mediamarkt.de': ('div', 'class', 'price'),
            'tchibo.de': ('span', 'itemprop', 'price'),
            'conrad.de': ('div', 'itemprop', 'price'),                              #has bot protection
            'amazon.de': ('span', 'id', 'priceblock_ourprice', 'priceblock_dealprice'),
            'otto.de': ('span', 'id', 'reducedPriceAmount', 'normalPriceAmount')}
        return switch.get(website)

    def createFolder(directory, folder):

        """
        Creates a folder in a given directory

        Args:
            directory: Location where the folder should be created
            folder: name of the folder
        """
        try:
            os.chdir(directory)
            if not os.path.exists(folder):
                os.makedirs(folder)
        except OSError:
            print ('Error: Creating directory. ' +  folder)

    def scrapePrices(driverArray):

        """
        Consists of three loops:
        -The outer loop will iterate through all specified sites
        -The middle loop will iterate through all given drivers from automisedProfileValidation
        -The inner loop will visit shoppin-urls as specified in the config, uses BeautifulSoup to scrape
        the sites context, filters for the price and uses Pandas to build a .csv containing a DataFrame
        of prices in relation to the url to analyse it later.

        Args:
            driverArray: Array of Selenium webdrivers generated by automisedProfileValidation

        Returns: saves dataframe.csv file for each specified website to the specified location of scraperResults
        """
        #load config
        _config = SafeConfigParser()
        _config.read('config.ini')
        _scrapeSitesArray = _config.get('scrape', 'pages').split()
        _scrapeDelay = _config.get('scrape', 'delay')
        _scrapeRepeats = _config.get('scrape', 'repeats')
        _profileDirectory = _config.get('directories', 'profileDirectory')
        _scrapeDirectory = _config.get('directories', 'scrapeDirectory')
        _cookieDirectory = _config.get('directories', 'cookieDirectory')
        _user = _config.get('user', 'used').split()

        for _ in range(int(_scrapeRepeats)):
            print("run " + str(_))
            _runIdentifier = "run " + str(datetime.datetime.now())[:19].replace(":", ".")

            for _scrape in _scrapeSitesArray:
                #This datalist will contain datalists of the users scraped product prices
                #Keys will be used to map the driverArray to a dictionary
                _metaDatalist = []
                _keys = []
                #for every user a webdriver will be started with the set Firefox profile and proxy from the config
                for _driver in driverArray:

                    #this datalist will contain the scraped prices of one user for each product as an array.
                    _datalist = []
                    #each website URL will be called
                    _url = "https://www." + _scrape

                    support.loader(_driver, _user[driverArray.index(_driver)], _scrape, _cookieDirectory)
                    time.sleep(2)

                    _products = _config.get('scrape', _scrape).split()

                    for _prod in _products:
                        try:
                            if (_prod[0]=="/"):
                                _driver.get(_url + _prod)
                            else:
                                _driver.get(_prod)
                        except TimeoutException:
                            print ("Loading took too much time!")
                        time.sleep(2)
                        #BeautifulSoup will scrape the Html source page and filter for set words individually for each site
                        _soup=BeautifulSoup(_driver.page_source, "html.parser")
                        #get the location of the price within the html structure
                        _scrapeLocation = priceWebScraper.scrapeLocationSwitch(_scrape)
                        _location = _scrapeLocation[0]
                        _type = _scrapeLocation[1]
                        _price = "00,00"

                        #if prices can be found by different names, e.g. reduced price is called "price orange"
                        #while normal price is called "price ", each will be looked for and the most relevant
                        #will loop at the end to overwrite previous false results
                        for _name in _scrapeLocation[2:]:
                            try:
                                _priceBox = _soup.find(_location, attrs={_type:_name})
                                #find the price and cut it by iterating to its start
                                _price = _priceBox.text
                                _price = "#" + _price #indicator for price starting
                                _middle = _price.find(",")
                                _loop = True
                                i = 0
                                while(_loop == True):
                                    if (_price[_middle-(i+1)] in ['0','1','2','3','4','5','6','7','8','9','.']):
                                        i = i + 1
                                    else:
                                        _loop = False
                                _price = _price[_middle-i:_middle+3]
                                _price = _price.replace("-","00")
                                _price = _price.replace(".","")
                            except Exception:
                                print ("No price found")
                        _datalist.append(_price)

                    _datetime = str(datetime.datetime.now())
                    _keys.append(driverArray.index(_driver))
                    _datalist.insert(0,_scrape)
                    _datalist.insert(0,_datetime)
                    _metaDatalist.append(_datalist)

                support.dumper(_driver, _user[driverArray.index(_driver)], _scrape, _cookieDirectory)
                _datetime = _datetime[:19]
                _datetime = _datetime.replace(":", ".")
                #Build a DataFrame, transpose it and save it using Pandas
                _products.insert(0, "site")
                _products.insert(0, "time")
                _df = pd.DataFrame(_metaDatalist, columns=_products)
                _df = _df.rename(index=dict(zip(_keys, _user)))
                _df = _df.T
                priceWebScraper.createFolder(_scrapeDirectory, _runIdentifier)
                os.chdir(_scrapeDirectory + "/" + _runIdentifier)
                _df.to_csv((_datetime + " " +_scrape + '.csv'), sep='\t', encoding='utf-8')
                print(_df)
                print("Dataframe for " + _scrape + " generated")
            if (_ < (int(_scrapeRepeats)-1)):
                time.sleep(int(_scrapeDelay))
